{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Importing Necessary Libraries, Loading, and Cleaning Documents"
      ],
      "metadata": {
        "id": "wSxABRrR-10m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS.remove('and')\n",
        "STOPWORDS.remove('or')\n",
        "STOPWORDS.remove('not')\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                documents[filename] = file.read()\n",
        "    return documents\n",
        "\n",
        "documents = load_documents('path_to_documents')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [LEMMATIZER.lemmatize(token) for token in tokens if token not in STOPWORDS]\n",
        "    return tokens\n",
        "\n",
        "cleaned_documents = {filename: clean_text(content) for filename, content in documents.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0RYG6KR84lr",
        "outputId": "2d3b98fc-ac3b-4548-f565-59ef10f544a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Creating Vocabulary"
      ],
      "metadata": {
        "id": "FnsJmnP985bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(cleaned_documents):\n",
        "    vocabulary = set()\n",
        "    for tokens in cleaned_documents.values():\n",
        "        vocabulary.update(tokens)\n",
        "    return vocabulary\n",
        "\n",
        "vocabulary = create_vocabulary(cleaned_documents)"
      ],
      "metadata": {
        "id": "rALrHDfY8re8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Computing Term Frequency"
      ],
      "metadata": {
        "id": "CuIYFJOR3pl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_term_frequency(cleaned_documents):\n",
        "    term_frequency = defaultdict(Counter)\n",
        "    for filename, tokens in cleaned_documents.items():\n",
        "        term_frequency[filename] = Counter(tokens)\n",
        "    return term_frequency\n",
        "\n",
        "term_frequency = compute_term_frequency(cleaned_documents)"
      ],
      "metadata": {
        "id": "4t9eC_iY2p9J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Compute Inverse Document Frequency"
      ],
      "metadata": {
        "id": "_AAz5f8Q3w9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_inverse_document_frequency(cleaned_documents):\n",
        "    num_documents = len(cleaned_documents)\n",
        "    df = Counter()\n",
        "\n",
        "    for tokens in cleaned_documents.values():\n",
        "        unique_tokens = set(tokens)\n",
        "        for token in unique_tokens:\n",
        "            df[token] += 1\n",
        "\n",
        "    idf = {token: math.log(num_documents / df[token]) for token in df}\n",
        "    return idf\n",
        "\n",
        "idf = compute_inverse_document_frequency(cleaned_documents)"
      ],
      "metadata": {
        "id": "gisqIJwn2rtc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Computing TF-IDF for Documents"
      ],
      "metadata": {
        "id": "vbZ22sHx34Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf_idf(term_frequency, idf):\n",
        "    tf_idf = defaultdict(dict)\n",
        "    for filename, tf in term_frequency.items():\n",
        "        for term, freq in tf.items():\n",
        "            tf_idf[filename][term] = freq * idf[term]\n",
        "    return tf_idf\n",
        "\n",
        "tf_idf = compute_tf_idf(term_frequency, idf)"
      ],
      "metadata": {
        "id": "5zu-Poa92w73"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Computing TF-IDF for Queries"
      ],
      "metadata": {
        "id": "v5cNFdlI6j1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_query(query):\n",
        "    return clean_text(query)"
      ],
      "metadata": {
        "id": "JsykjLLv6g0E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_query_tf_idf(query, idf):\n",
        "    tokens = clean_query(query)\n",
        "    tf = Counter(tokens)\n",
        "    query_tf_idf = {term: freq * idf.get(term, 0) for term, freq in tf.items()}\n",
        "    return query_tf_idf"
      ],
      "metadata": {
        "id": "94s3QaTg6nvF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Computing Cosine Similarity Between Documents and Queries"
      ],
      "metadata": {
        "id": "jck3AL4g7tlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "    intersection = set(vec_a) & set(vec_b)\n",
        "    numerator = sum(vec_a[x] * vec_b[x] for x in intersection)\n",
        "\n",
        "    sum_a = sum(vec_a[x] ** 2 for x in vec_a)\n",
        "    sum_b = sum(vec_b[x] ** 2 for x in vec_b)\n",
        "    denominator = math.sqrt(sum_a) * math.sqrt(sum_b)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "r8ulzmtI6tH9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Example queries"
      ],
      "metadata": {
        "id": "SAAgTm9K75XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"storm heart\",\n",
        "    \"robert\",\n",
        "    \"sun\"\n",
        "]\n",
        "\n",
        "# Compute TF-IDF for each query and cosine similarity with documents\n",
        "query_tf_idf = {query: compute_query_tf_idf(query, idf) for query in queries}\n",
        "\n",
        "# Compute cosine similarities between each query and each document\n",
        "cosine_similarities_queries = defaultdict(dict)\n",
        "for query, query_vector in query_tf_idf.items():\n",
        "    for doc_name, doc_vector in tf_idf.items():\n",
        "        similarity = cosine_similarity(query_vector, doc_vector)\n",
        "        cosine_similarities_queries[query][doc_name] = similarity"
      ],
      "metadata": {
        "id": "ZWquTXBX6tzn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Writing Results to a Text File"
      ],
      "metadata": {
        "id": "QSfeiqGo8B5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"result_soniya.txt\", \"w\") as result_file:\n",
        "    result_file.write(\"\\nCosine Similarities (Queries to Documents):\\n\")\n",
        "\n",
        "    for query, similarities in cosine_similarities_queries.items():\n",
        "        result_file.write(f\"\\nRanked Cosine Similarities for Query: '{query}':\\n\")\n",
        "\n",
        "        # Sort the similarities in descending order based on the similarity score\n",
        "        ranked_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Write the ranked results to the file\n",
        "        for rank, (doc_name, similarity) in enumerate(ranked_similarities, start=1):\n",
        "            result_file.write(f\"Rank {rank}: Cosine similarity between Query: '{query}' and Document: '{doc_name}': {similarity}\\n\")"
      ],
      "metadata": {
        "id": "pKGRTzL761fm"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}